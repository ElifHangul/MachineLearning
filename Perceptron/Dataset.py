# Import libraries
import numpy as np
import matplotlib.pyplot as plt
from Perceptron import Perceptron
from mlxtend.plotting import plot_decision_regions

# create Perceptron class objects to use. 
p = Perceptron(100, 1) # used for sample size 20 generated by hand
p2 = Perceptron(100, 1) # used for sample size 20 generated randomly
p3 = Perceptron(200, 1) # used for sample size 100 generated randomly
p4 = Perceptron(300, 1) # used for sample size 1000 generated randomly
p5 = Perceptron(1000, 1) # used for sample size 1000 with 10 dimensional data generated randomly

# Question part a
# Generate a dataset of size 20. Plot the examples[(xn,yn)] as well as the target function f on a plane.

# target array for 20 input. Mapping -1 or 1. For simplicity half the points map to -1 and other half maps to 1 
targets = np.array([-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,1,1,1,1,1,1,1,1,1,1]) 
# generate sample data. The points that maps to -1 are denoted by neg as in negative while the points map to 1 denoted by pos as in positive 
neg = np.array([
    [-2,4],
    [-4,3],
    [-3,2],
    [-2,1],
    [-4,4],
    [-2,2],
    [-3,4],
    [-3,1],
    [-4,1],
    [-2,2],])
pos = np.array([ 
   
    [2,4],
    [3,4],
    [4,3],
    [4,4],
    [3,2],
    [2,2],
    [3,3],
    [4,2],
    [4, 1],
    [3, 1]])

# Prepare input data by combining negative and positive points
X = np.concatenate((neg,pos))
# assign targets array to y value
y=targets
# x_length is the length of X array in this case 20
x_length= len(X) 

# Plotting the points. As by design there are 10 negative and 10 positive points.
for s, sample in enumerate(X): 
# Plot the negative samples with color blue

    if s < (x_length/2): 
        plt.scatter(sample[0], sample[1], c="blue") 
        
    # Plot the positive samples with color yellow
    else: 
        plt.scatter(sample[0], sample[1], c="yellow") 
        
plt.xlabel('X') 
plt.ylabel('Y')
# The target function f  
plt.plot([-1,0],[0,5], "g")
# Part a is finished.
plt.show()

# Ouestion part b
# Run the perceptron algorithm on the dataset. Report the number of updates that the algorithm takes before converging. Plot the examples
# [(xn,yn)], the target function f, and the final hypothesis g in the same figure.
# generated the labels

# Now we run perceptron on the dataset.
# Call the train function from Perceptron class with X and y parameters to run the perceptron algorithm on the dataset and return the trained model
trained_model = p.train(X, y)

# use mlxtend library plot_decision_regions feature to plot the trained_model, assign the label and show them
plot_decision_regions(X, y.astype(np.integer), clf=trained_model)
plt.title('Perceptron')
plt.xlabel('X1')
plt.ylabel('X2')
# A line that separates the points is found and market with a green color 
plt.plot([-1,0],[0,5], "g")
plt.show()   
# Print the number of updates it takes to converge
print("Number of updates till convergence (size: 20 data: generated by hand)", p.counter)

# Question part c
# Repeat everything in b) with another randomly generated dataset of size 20, and compare the result to b)

# starting with creating the targets array. 10 negative ones and 10 positive ones are created and combined.
negones = np.zeros(10)-1
ones = negones+2
targets = np.concatenate((negones, ones))


# Generate data randomly. 
# Data is selected by normal distribution with spread of 0.25
# 10 data is mapped to -1 and 10 mapped to 1
# Data is 2 dimensional
neg = np.random.normal(4, 0.25, (10,2))
pos = np.random.normal(6.5, 0.25, (10,2))

# Prepare input data
X = np.concatenate((neg,pos))
y=targets
trained_model = p2.train(X, y)

plot_decision_regions(X, y.astype(np.integer), clf=trained_model)
plt.title('Perceptron')
plt.xlabel('X1')
plt.ylabel('X2')
# A line that separates the points is found and market with a green color 
plt.plot([5.5,5.8],[3,8], "g")
plt.show()
# Print the number of updates it takes to converge
print("Number of updates till convergence (size: 20 data: generated randomly)", p2.counter)

# Question part d
# Repeat everything in b) with another randomly generated dataset of size 100, and compare the results to b)

#input size 100
#Generate targets
negones = np.zeros(50)-1
ones = negones + 2
targets = np.concatenate((negones, ones))


# Generate data
neg = np.random.normal(4, 0.25, (50,2))
pos = np.random.normal(6.5, 0.25, (50,2))

# Prepare input data
X = np.concatenate((neg,pos))
y=targets
trained_model = p3.train(X, y)

plot_decision_regions(X, y.astype(np.integer), clf=trained_model)
plt.title('Perceptron')
plt.xlabel('X1')
plt.ylabel('X2')
# A line that separates the points is found and market with a green color 
plt.plot([5.5,5.8],[3,8], "g")
plt.show()
# Print the number of updates it takes to converge
print("Number of updates till convergence (size: 100 data: generated randomly)", p3.counter)

# Question part e
# Repeat everything in b) with another randomly generated dataset of size 1000., and compare the results to b)

#input size 1000
# Generate targets
negones = np.zeros(500)-1
ones = negones + 2
targets = np.concatenate((negones, ones))


# Generate data
neg = np.random.normal(4, 0.25, (500,2))
pos = np.random.normal(6.5, 0.25, (500,2))

# Prepare input data
X = np.concatenate((neg,pos))
y=targets
trained_model = p4.train(X, y)

plot_decision_regions(X, y.astype(np.integer), clf=trained_model)
plt.title('Perceptron')
plt.xlabel('X1')
plt.ylabel('X2')
# A line that separates the points is found and market with a green color 
plt.plot([5.2,5.8],[2.5,8], "g")
plt.show()
# Print the number of updates it takes to converge
print("Number of updates till convergence (size: 1000 data: generated randomly)", p4.counter)    


# Question part f
# Modify the experiment such that dataset has 10 dimensions. Run the algorithm on a randomly generated dataset of size 1000. How many updates
# does the algorithm take to converge.

# Generate targets    
negones = np.zeros(500)-1
ones = negones+2
targets = np.concatenate((negones, ones))


# Generate data with 10 dimensions
neg = np.random.normal(4, 0.25, (500,10))
pos = np.random.normal(6.5, 0.25, (500,10))

# Prepare input data
X = np.concatenate((neg,pos))
y=targets
# Essentially we use the same algorithm. In the train methos weight array is calculated by the number of features X has. 
# Therefore 10 dimension only makes the weight vector size of 11.(1 more is added for bias)
trained_model = p5.train(X, y)
# Then we print the number of updates till convergence normally
print("Number of updates till convergence (size: 1000 data: generated randomly)", p5.counter)  
